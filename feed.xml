<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gtangg12.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gtangg12.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-23T23:01:54+00:00</updated><id>https://gtangg12.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Ocean Image Formation</title><link href="https://gtangg12.github.io/blog/2024/ocean-image-formation/" rel="alternate" type="text/html" title="Ocean Image Formation"/><published>2024-11-30T12:00:00+00:00</published><updated>2024-11-30T12:00:00+00:00</updated><id>https://gtangg12.github.io/blog/2024/ocean-image-formation</id><content type="html" xml:base="https://gtangg12.github.io/blog/2024/ocean-image-formation/"><![CDATA[<p>The problem of light transport on land is relatively simple and well-modeled. However, the same cannot be said for the ocean, a complex medium with a variety of factors that affect light transport such as absorption and scattering of light by water, the reflection of light off the ocean surface, the influence of waves on the process, to name a few. In this post, we will explore how light transport in the ocean can be modeled and how this can be used to generate realistic ariel images of objects in the ocean with the ultimate goal of recovering a view of the object without the ocean.</p> <p>I included a juypter notebook here that can be run as you read along <a href="https://github.com/gtangg12/image-restoration/blob/main/notebooks/corruption_ocean.ipynb">here</a>, from which you can find implementations for the methods described below.</p> <h2 id="wave-formation">Wave Formation</h2> <p>We first describe how waves are formed in the ocean. Specifically, consider a \(L_x\) by \(L_z\) (m) size patch modeled by a grid at resoluion \(N\) x \(M\). Each entry in the grid contains at time \(t\) a ocean surface height value \(h(x, z, t)\), a unit normal \(n(x, z, t)\), and a position \(r(x, z, t)\). The movie Titanic employed the Fast Fourier Transform (FFT) method to generate waves described in [1]. Specifically the ocean surface height \(h(x, z, t)\) is modeled as a sum of sinusoids with random phases and amplitudes.</p> <p>First let’s consider our patch has size \(N\) and \(M\) at the same resolution. We can write the heightmap as</p> \[h(x, z, t) = \sum_{n=0}^{N-1}\sum_{m=0}^{M-1} \tilde{h}(n, m, t) e^{2\pi i\left(\frac{nx}{N} + \frac{mz}{M}\right)}\] <p>where \(\tilde{h}(n, m, t)\) is the Fourier amplitude of the wave at frequency \(m, n\). We will discuss how to obtain the Fourier amplitude later. We can scale the Fourier transform to any size \(L_x\), \(L_z\) using the scaling property, where \(f\) denotes the Euclidean domain and \(F\) frequency domain and \(a, b\) are the scaling factors</p> \[f(ax, by) = \frac{1}{|ab|}F\left(\frac{u}{a}, \frac{v}{b}\right)\] <p>from which we get</p> \[h(x, z, t) = S \sum_{n=0}^{N-1}\sum_{m=0}^{M-1} \tilde{h}(n, m, t) e^{2\pi i\left(\frac{nx}{L_x} + \frac{mz}{L_z}\right)}\] <p>where \(S = (NM)/(L_xL_z)\). The position can be expressed as</p> \[r(x, z, t) = \left( x, h(x, z, t), z\right)\] <p>The slope is given by the gradient of the height field, which can also be computed via FFT</p> \[\begin{align*} e(x, z, t) &amp;=S \ \nabla h(x, z, t) =S\sum_{n=0}^{N-1}\sum_{m=0}^{M-1} 2\pi i \begin{bmatrix} n / L_x \\ m / L_z \end{bmatrix} \tilde{h}(n, m, t) e^{2\pi i\left(\frac{nx}{L_x} + \frac{mz}{L_z}\right)} \end{align*}\] <p>Let \(\hat{y}\) be the unit up vector. The normal is given by</p> \[n(x, z, t) = \hat{y} - \frac{e(x, z, t)}{\sqrt{1 + \|e(x, z, t)\|^2}}\] <p>Statisitcal analysis of waves have shown the Fourier amplitudes h(k, t) can be modeled by gaussian random variables according to the wave spectrum \(P_h(m, n)\) at time 0 as</p> \[\tilde{h}_0(m, n) = \sqrt{\frac{P_h(k)}{2}}(\mathcal{E}_1 + i\mathcal{E}_2)\] <p>where \(\mathcal{E}_1, \mathcal{E}_2\) are independent gaussians with zero mean and unit variance. Note since \(h\) is real, \(\tilde{h}(n, m, t) = \tilde{h}(N-n, M-m, t)\). Let \(w\) denote the wind vector. We specifically use the Phillips Spectrum, though other spectra can be used. The Phillips spectrum is given by</p> \[P_h(m, n) = \frac{A\exp\left(-1 / (kL)^2\right)}{k^4}\cos(\theta_{w})^p\] <p>where \(A\) is the Phillips constant, \(k = 2\pi\sqrt{(n / Lx)^2 + (m / L_z)^2}\) the wave vector, \(L = \frac{\|w\|^2}{g}\), \(g\) is gravity, \(\theta_{w}\) is the angle between the wind vector and the wave vector, and \(p\) controls the alignment between the wind and wave vectors e.g. 2 or 6. . This model has poor convergence properties at high values of the wavenumber \(k\). A simple fix is to suppress waves smaller that a small length \(\mathcal{l} &lt; L\), and modify the Phillips spectrum by a damping factor \(\exp\left(-k^2\mathcal{l}^2\right)\).</p> <p>Given a dispersion relation \(\omega(m, n)\), which describes how to wave at frequency \(m, n\) evolves over time, we can write</p> \[\tilde{h}(m, n, t) = \tilde{h}_0(m, n)e^{i\omega(m, n)t}\] <p>Note if we use \(\texttt{torch.fft.irfft2}\), the conjugation property is preserved as the negative frequencies of the last dimension are ignored. The dispersion relation is given by</p> \[\omega^2(m, n) = gk\tanh(kD)\] <p>for water of depth \(D\). For deep water, we can simplfy the expression as</p> \[\omega^2(m, n) = gk\] <p>We show some examples of wave height and normal maps at different resolutions.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap_resolutions1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"></figcaption> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap_resolutions2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Height/normal maps of 64 x 64m patches at different resolutions. The effect of resolution is best viewed when zoomed in.</figcaption> </figure> </div> <h4 id="parameter-choices">Parameter Choices</h4> <p>To quote directly from [1], the values of N and M can be between 16 and 2048, in powers of two. For many situations, values in the range 128 to 512 are sufficient. For detailed surfaces, 1024 and 2048 can be used. For example, the wave fields used in the motion pictures Waterworld and Titanic were 2048 × 2048 in size, with the spacing between grid points at about 3 cm. Above a value of 2048, one should be careful because the limits of numerical accuracy for floating point calculations can become noticeable. Since the wave behvaior below 1cm is not well modeled, \(\mathcal{l}\) can be set to something around that value.</p> <p>Below is an example of a wave map generated at 1024 x 1024 resolution of a 64 x 64m patch of ocean. We set the Phillips constant to \(A=256\), use an alignment of \(\cos^6\), and supress waves smaller than \(10^{-3}\)cm.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Wave height map generated from FFT method with wind &lt;20, 20&gt; at time 0.</figcaption> </figure> </div> <h2 id="reflection-refraction-absorption-and-scattering">Reflection, Refraction, Absorption, and Scattering</h2> <p>The ocean is a complex medium that can reflect, refract, absorb, and scatter light. The Fresnel equations describe how light is reflected and refracted at the ocean surface. Reflection is given by</p> \[\hat{n}_r = \hat{n}_i - 2(\hat{n}_i \cdot \hat{n})\hat{n}\] <p>where \(\hat{n}_r\) is the reflected normal, \(\hat{n}_i\) is the incident normal, and \(\hat{n}\) is the surface normal.</p> <p>Transmission is governed by refraction of light through water, which is given by</p> \[\hat{n}_t = \frac{\eta_1}{\eta_2}\hat{n}_i - \left(\frac{\eta_1}{\eta_2}\hat{n}_i \cdot \hat{n} + \sqrt{1 - \left(\frac{\eta_1}{\eta_2}\right)^2(1 - (\hat{n}_i \cdot \hat{n})^2)}\right)\hat{n}\] <p>where \(\eta_1\) is the refractive index of air and \(\eta_2\) is the refractive index of water. The refractive index of air is defined to be 1 and water around 1.33. Note, there is a case where total internal reflection occurs, which is when the angle of incidence is greater than the critical angle. In this case, the light is reflected back into the water. The critical angle is given by</p> \[\theta_c = \arcsin\left(\frac{\eta_2}{\eta_1}\right)\] <p>The Fresnel equation determine the transmission, \(T\) and reflection \(R\) coefficients, which are multipled by the incident light for each scenario. As given in [1]</p> \[R = \frac{1}{2}\left( \frac{\sin^2(\theta_t - \theta_i)}{\sin^2(\theta_t + \theta_i)} + \frac{\tan^2(\theta_t - \theta_i)}{\tan^2(\theta_t + \theta_i)} \right)\] <p>where \(\theta_i\) is the angle of incidence and \(\theta_t\) is the angle of transmission. The transmission coefficient is given by</p> \[T = 1 - R\] <p>by conservation of energy.</p> <p>Since water is a absorbing as well as scattering medium, we can approximately model the attenuation of light by Koschmieder [2]</p> \[L = L_0 e^{-\beta d} + \alpha (1 - e^{-\beta d})\] <p>where \(L\) is the resulting luminance, \(L_0\) luminance of directional light, \(\beta\) scattering coefficent, \(d\) is the distance between the ocean surface intersection point and the ocean floor.</p> <p>Furthermore, to model the fact that absorption of different colors of light depends on depth, we linearly interpolate between a uniform albedo and deep water albedo between depths 0 and \(d\) (m) and beyond \(d\) we use the deep water albedo.</p> <h2 id="two-stage-light-transport">Two Stage Light Transport</h2> <p>We describe a two stage light transport model for ocean imaging. The first stage is the light transport from the light source (directional light from the sun) to illuminate the ocean bottom surface. The second stage is the light transport from the ocean surface to the camera.</p> <p>In the first stage, we compute a lightmap of the ocean floor, where each pixel \((x, z)\) corresponds to the color of the ocean floor at \((x, z)\). We begin by tracing directional light to the wave, refracting it, and then tracing it to the respective point on the ocean floor where it is accumulated, producing a radiance map. Tracing an be vectorized by solving for the positions on the ocean bottom where the ray ends. To account for attenuation, we also need the distance the ray has traveled. Assuming constant depth for now, both can be handled via ray-plane intersection. We will discuss the more complicated varying depth case in the next subsection.</p> <p>The above method can produce realistic caustic effects. Some points may not recieve light due to the limited resolution of the grid, however, we can take an max pool to smooth out the radiance map. The ocean bottom texture map is multipled by the radiance map to get the lightmap.</p> <p>The next stage, we can trace rays from the camera to the ocean surface, refract them (refraction is symmetrical wrt to time-reversal), and then trace them to the ocean floor. The corresponding lightmap value is traced and attenuated again back to the surface and accumulated at the corresponding pixel.</p> <p>Below we show from left to right the original image, bottom lightmap, and final aerial image generated from the process for the wavemap shown before assuming constant depth of the ocean surface. Notice the caustic effects in the lightmap and final image, which we term the corrupted image since our ultimate goal is to see through the ocean.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap1_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 150%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="nonconstant-depth">Nonconstant Depth</h4> <p>The constant depth case takes \(O(1)\) time and \(O(HW)\) memory on a parallel computation model e.g. GPU when performing vectorized ray-plane intersection. Dealing with nonconstant depth is tricky because the ray might terminate before it hits the plane defined by the min depth. We showcase an implementation that \(O(1)\) time and \(O(HW\text{nsteps})\) memory where \(\text{nsteps}\) is defined as the maximal number of steps a ray will take at a resolution until it terminates. The idea is we discretize the ray’s path into nsteps and check for intersections at each step by seeing if the position of the ray is above the depth. The resolution is determined as</p> \[\delta = \min(dx, dz) = \min(L_x/N, L_z/M)\] <p>since any ray must travel at least \(\delta\) distance to cross a cube of dimensions \(dx \times dz \times \epsilon\) for any \(\epsilon\). We can upper bound \(\text{nsteps}\) with \(d_{\min} / \delta\) where \(d_{\min}\) denotes the distance to the plane defined by the min depth.</p> <p>However, this process may result in excessive memory usage. We can convert memory into time by accumulating over depth range batches resulting in \(O(B)\) time and \(O(HW\text{nsteps}/B)\) memory. \(B = 2\) is sufficient for up to \(25\)m depth.</p> <p>Below we show a few examples of nonconstant depth.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap2_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>In the above example, we observe artifacts at the end due to assuming repeatedly tiled ocean bottom for depth and texture as well as wave patch.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap3_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="secondary-reflections">Secondary Reflections</h4> <p>Reflection in the ocean is very complex as not only do we need to consider direct reflection of directional light, but also secondary reflection from waves. We model reflection as a power function</p> \[R \cdot I \cdot \alpha(\hat{n}_r \cdot \hat{r})^\beta\] <p>where \(R\) denotes the reflection coefficient, \(I\) light intensity, and \(\hat{r}\) the camera ray. \(\alpha, \beta\) can be tuned and define the power function. Intuitively, higher \(\beta\) sharpens the reflection i.e. less water exhibits specular properties.</p> <p>Putting everything together we are able to obtain some examples under different parameter settings (zoom in to see the details)</p> <p>Direct sunlight from above</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap4_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Inclined sunlight direction</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap5_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>We can decrease the patch size and increase the wave dampening to get cool distortion effects.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/ocean-image-formation/wavemap6_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Thank you for reading! This post will be updated soon with a solution to the inverse graphics problem of removing the ocean effects from the image.</p> <h2 id="references">References</h2> <p>[1] Tessendorf J., Simulating Ocean Water, 2004 <br/> [2] Koschmieder, H., Theorie der horizontalen Sichtweite. Beitr. Phys. Atmos. 12, 33-58, 1906</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="computer-graphics"/><category term="physical-simulation"/><summary type="html"><![CDATA[How can we model light transport in the ocean?]]></summary></entry><entry><title type="html">Vision in Degraded Visibility Conditions</title><link href="https://gtangg12.github.io/blog/2024/vision-through-scattering-medium/" rel="alternate" type="text/html" title="Vision in Degraded Visibility Conditions"/><published>2024-10-18T12:00:00+00:00</published><updated>2024-10-18T12:00:00+00:00</updated><id>https://gtangg12.github.io/blog/2024/vision-through-scattering-medium</id><content type="html" xml:base="https://gtangg12.github.io/blog/2024/vision-through-scattering-medium/"><![CDATA[<p>I have been working on vision in degraded visibility conditions (fog, rain, snow, and nighttime) for a long time. Over the years I have made progress on many facets the problem, and many things in the field of vision has also changed along the way. I will outline some interesting aspects in this post.</p> <h2 id="introduction-to-atmospheric-scattering">Introduction to Atmospheric Scattering</h2> <p>To motivate the problem, consider the scenario where you are given a single foggy RGB image and you want to obtain the defogged image. Before we delve into the details, let’s first understand the physics of fog. Fog is a weather phenomenon that occurs when the air is saturated with water vapor. The water droplets in the air scatter light (a phenomenon known as Mie scattering), which reduces the contrast and visibility of objects in the scene. Let \(L\) be the luminance of an object under monochromatic light, \(L_0\) the luminance at zero distance, \(\beta\) Mie scattering parameter, and \(\alpha\) background luminance (airlight). Both Koschmieder (1924) [1] and Duntley (1948) [2] showed the scattering of light is modeled by the following equation (Atmospheric Scattering Model):</p> \[L = L_0 e^{-\beta d} + \alpha (1 - e^{-\beta d})\] <p>where \(d\) is the distance between the observer and the object. This model is applicable to visibility degredation through a scattering medium in general, and not limited to fog.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/asm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Atmospheric Scattering Model</figcaption> </figure> </div> <p>Humans are trichromatic and have cones for short, medium, and long wavelengths that correspond to our perception of blue, green, and red primary colors (1931 CIE color matching). The luminance of channel \(y\) is computed by integrated the respective color matching function \(y(\lambda)\) against the irradiance, \(E\), over all wavelengths.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/cie.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Human sensativity to different wavelengths for each cone.</figcaption> </figure> </div> <p>Without scattering, the luminance of each channel is given by</p> \[L_{y} = \int_{0}^{\infty} y(\lambda) E(\lambda) d\lambda\] <p>Mie scattering is independent of wavelengths, that is with scattering, the luminance of each channel is given by the Atmospheric Scattering Model as</p> \[L_{y} = \int_{0}^{\infty} y(\lambda) \left[ E(\lambda) e^{-\beta d} + \alpha (1 - e^{-\beta d}) \right] d\lambda\] <p>As the color matching functions are normalized to integrate to the same constant, and the transformation from CIE RGB to standard RGB colorspace is linear, for each of the RGB channels of the image we have</p> \[I(x) = J(x) t(x) + \alpha(1 - t(x))\] <p>where \(I(x)\) is the observed luminance at pixel \(x\), \(J(x)\) is the scene luminance, \(t(x) = e^{-\beta d(x)}\) is the transmission map, \(d(x)\) the depth map, and we absorb the normalization constant into \(\alpha\). With some algebra, we can see to estimate the scene radiance \(J(x)\) we need to estimate the transmission map \(t(x)\).</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/asm_examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Visibility Degredation for varying levels of parameters. Figure credits to Neal Bayya</figcaption> </figure> </div> <p>Two early works, DehazeNet [3] and Dark Channel Prior [4], respectively estimated the transmission map directly in an end-to-end manner using a ConvNet and using the Dark Channel Prior. Once we have the transmission map, as in DehazeNet, \(\alpha\) can be estimated as</p> \[\alpha = \max_{y, x \in t(x) &lt; t_0} I(x)\] <p>where \(t_0\) is a tunable parameter.</p> <h2 id="visibility-distance-estimation">Visibility Distance Estimation</h2> <p>While at Nvidia Self Driving, I worked on operational domain verification for autonomous vehicles. One of the tasks was to determine the <em>meteorological</em> visibility distance, which influenced the decisions made by the perception stack. We can rearrange the Atmospheric scattering model as follows</p> \[\frac{L - \alpha}{\alpha} = \left( \frac{L_{0} - \alpha}{\alpha} \right)e^{-\beta d}\] <p>or equivalently</p> \[C = C_{0}e^{-\beta d}\] <p>where \(C\) is defined as the visual contrast. The meteorological visibility distance is defined as the distance at which the visual contrast is reduced to a certain threshold, usually 0.05 of a black object (\(L_{0} = 0\)), which is approximately</p> \[d_{m} = -\frac{\ln(0.05)}{\beta}\] <p>It is important to note visibility distance degredation is a global effect. This means local effects by lamppost glare, puddle reflections, vehicle spray do not contribute to the visibility distance. This does not mean we do not consider them; on the contrary, our model must learn how to recongize and ignore local effects. The team decided to train a CNN to regress visibility distance from synthetic data, as real images in degraded conditions were sparse due to collection bias.</p> <p>If the only condition considered was fog, the solution would be straightforwards. In the real world, however, degraded visibility is usually observed with other weather conditions such as rain, sleet, or snow, as shown below</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_conditions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Visibility can be influenced globally or locally.</figcaption> </figure> </div> <p>The team decided to generate data using DriveSim, Nvidia’s simulation engine, with global effects e.g. fog, rain, snow, sleet, or nighttime as well as local effects. Some examples of global and local effects at a fixed visibility degredation are shown below</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_sim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DriveSim can simulate a variety of different conditions.</figcaption> </figure> </div> <p>To get the ground truth visibility from global effect parameters, we setup an interface with simple track scene with an black object. The interface would binary search for where the user can detect the object</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_sim_interface.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Interface to map global effect parameters to visibility distance</figcaption> </figure> </div> <p>One issue however, was there is a large sim2real gap. Forunately, other projects had shown mixing real data with DriveSim data resulted the model generalizing well to real world data features. We decided to mix the DriveSim data with real visibility degraded data generated using the Atmospheric Scattering Model, estimating the depth maps via monocular depth estimation.</p> <p>We show two examples of real foggy images collected from real life and a visibility degraded image generated from a real clear image below</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_fog.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The atmospheric model is quite powerful and can produce varying levels of degredation</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_fog_levels.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Generated data can also be fed into a general data augmentation engine to produce different scenarios such as sand and dust storms</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/visibility_diverse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Verification with real data collected by test drives produced approximately reasonable results. In practice, the regressed visibility distance is classified into bins, and the car correspondingly switches autonomy levels based on the designated visibility bin.</p> <h2 id="in-the-generative-model-era">In the Generative Model Era</h2> <p>The advent of generative diffusion processes opened up new avenues to the data augmentation aspect of the problem, specifically domain translation for autonomous vehicle dataset augmentation. Given a biased dataset of images collected under good driving conditions, can we construct foggy, rainy, snowy, and nighttime counterparts for each image, preserving structure and semantics but changing weather? Previous approaches such as CycleGAN and MUNIT have touched on this problem, but diffusion models provide an unprecented level of realism and control over the generated images.</p> <p>I constructed a language model-based augmentation agent that performs rejection sampling of InstructPix2Pix [5]. Given an input image, the language model would propose edits that InstructPix2Pix carried out. InstructPix2Pix often fails to generate edits or corrupts the image— edited images below an LPIPs threshold are removed. The agent then proceeds recursively to the accepted edits. Optionally as a final step, the depth map of the input image is estimated and visibility degradation is applied using an atmospheric scattering model. The pipeline is illustrated below</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/augmentation_agent_pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 75%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Recursive branching combined with rejection sampling can be effective at producing diverse augmentations from a given image.</figcaption> </figure> </div> <p>Here are some results of the agent, which is able to support a diverse array of quality edits far beyond what previous methods could achieve</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/augmentation_agent.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">InstructPix2Pix is good at producing structural edits.</figcaption> </figure> </div> <p>In general, the performance is decent but there are several issues with InstructPix2Pix:</p> <ul> <li>high corruption rate</li> <li>edit quality dependent on finetuning data</li> <li>does not support compositional domain translation e.g. a clear image is transformed to have a hybrid of weather conditions</li> </ul> <p>where the last point is particularly important.</p> <p>This led me to an alternative method: Schrodinger Bridge-based methods [6], [7], which are well grounded in theory and result in minimal corruption. They only rely on the diffusion model \(p_{\phi}(x)\) accurately modeling the source and target domain image distributions, compared to being finetuned on edits. In the case of [7] are defined as the diffusion model image distribution conditioned on keywords describing the source and target domains \(p_{\phi}(x, y_{\text{src}})\), \(p_{\phi}(x, y_{\text{tgt}})\). Given this setup, [7] provides a translation gradient to optimize the image \(x\) from the source to the target distribution</p> \[\nabla_x \mathcal{L}(x) \propto \mathbb{E}_{t, \epsilon}\left[ \epsilon_{\phi}(x_t, y_{\text{src}}, t) - \epsilon_{\phi}(x_t, y_{\text{tgt}}, t) \right]\] <p>where \(x_t\) denotes the noised image and \(\epsilon\) denotes the learned score function for \(p_{\phi}(x)\). Below are some videos of domain translation in action for snow, nighttime, and rain. The edits are more realistic than InstructPix2Pix since Schrodinger Bridge-based methods are based on a notion of optimal transport between the conditional diffusion distributions.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/augmentation_sds.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The generated images are more realistic considering context compared to InstructPix2Pix edits.</figcaption> </figure> </div> <p>Simply changing the target domain’s text conditioning is not stable nor controllable. We found that simply following a linear combination of translation gradients between well defined domains can achieve compositional domain translation. The weights, however, may not be proportionally to the desired influence of the domain. As an example, suppose we are translating to both domains A and B. The translation gradient for domain A may need to be weighted higher than the translation gradient for domain B for the final image to have A and B equally present. We propose to translate the image to domains A and B separately. Then, we dynamically adjust the weights during compositional domain translation optimization using an image similarity model such as CLIP by comparing the similarity of the currently optimized image to the separately translated images. Instead of weight adjustment, assuming there is a large number of optimization steps, each step we can step towards one domain, adjusting the target as needed. Below we show some examples of interpolations between night and snow with different weights.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/augmentation_sds_compositional.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Compositional domain translation, one step per iteration.</figcaption> </figure> </div> <h4 id="schrodinger-bridge-based-methods">Schrodinger Bridge-based Methods</h4> <p>The theory behind Schrodinger Bridge-based methods is quite rich. Let \(\Omega = \mathbb{C}([0, 1], \mathbb{R}^n)\) denote the space of <em>continuous</em> paths over \([0, 1]\). Given \(W\), a reference probability measure on \(\Omega\), the Schrodinger Bridge Problem (SBP) seeks to find a measure \(P_{SBP}\) such that</p> \[P_{SBP} = \arg\min_{P} D_{KL}(P||W)\] <p>for \(P \in D(p_0, p_1)\), where \(D\) denotes the collection of measures over \(\mathbb{C}\) with marginals \(p_0\) and \(p_1\).</p> <p>SBP has connections to diffusion processes. When \(W\) results from the forward SDE of the variance exploding (VE) score generative model (SGM) that models \(p_{\text{data}}\) and results in \(p_{\text{prior}}\), SBP can be shown to produce solutions to an entropy-regularized Monge-Kantorovich (distribution mass) optimal transport problem between \(p_0\) and \(p_1\) [8]. In fact this shows that SGM themselves are solutions to entropy-regularized optimal transport with 0 KL divergence since we can set \(p_0 = p_{\text{data}}\) and \(p_1 = p_{\text{prior}}\). Note any VP SDE can be reparmetrized as a VE SDE.</p> <p>To see this more clearly, note we can write the VE SDE forward and reverse processes as</p> \[\begin{align*} d\mathbf{x} &amp;= \mathbf{f}(\mathbf{x}, t) \, dt + g(t) \, d\mathbf{w}, \\ d\mathbf{x} &amp;= \left[\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right] \, dt + g(t) \, d\mathbf{w} \end{align*}\] <p>and SBP solution has forward and reverse processes of the form</p> \[\begin{align*} d\mathbf{x} &amp;= \left[\mathbf{f}(\mathbf{x}, t) + g(t)^2 \nabla_{\mathbf{x}} \log \Phi_t(\mathbf{x}) \right] \, dt + g(t) \, d\mathbf{w}, \\ d\mathbf{x} &amp;= \left[\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_{\mathbf{x}} \log \hat{\Phi}_t(\mathbf{x}) \right] \, dt + g(t) \, d\mathbf{w} \end{align*}\] <p>where \(\mathbf{f}\) and \(g\) refer to the drift and diffusion terms in the VE SDE and has marginals \(p_t(\mathbf{x}) = \Phi_t(\mathbf{x})\hat{\Phi}_t(\mathbf{x})\) where \(\Phi_t(\mathbf{x})\) and \(\hat{\Phi}_t(\mathbf{x})\) are called <em>Schrodinger factors</em> the solution to certain PDEs involving \(\mathbf{f}\) and \(g\) and the boundary conditions \(p_0\) and \(p_1\) [9].</p> <p>Let \(\mathbf{z}_t = g(t)\nabla_{\mathbf{x}}\log \Phi_t(\mathbf{x})\) and \(\hat{\mathbf{z}}_t = g(t)\nabla_{\mathbf{x}}\log \hat{\Phi}_t(\mathbf{x})\). Wen \(p_0 = p_{\text{data}}\) and \(p_1 = p_{\text{prior}}\), the Schrodinger factors are</p> \[(\mathbf{z}_t, \hat{\mathbf{z}}_t) = (0, g(t)\nabla_{\mathbf{x}} \log p_t(\mathbf{x}))\] <p>which is exactly the SGM [9]. Each SBP SDE also has a corresponding ODE is a solution to the SBP and has the same marginals \(p_t\) [9]. Thus it also satisfies the optimal transport criteria.</p> \[d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) + g(t)\mathbf{\mathbf{z}_t} - \frac{1}{2}g(t)\left( {\mathbf{z}_t + \hat{\mathbf{z}}_t} \right) \right]dt\] <p>Substituting \((\mathbf{z}_t, \hat{\mathbf{z}}_t)\) from above, we get the probabability flow ODE for the SGM</p> \[d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g(t)^2\nabla_{\mathbf{x}}\log p_t(\mathbf{x}) \right]dt\] <p>which has the same marginals as the VE SDE and is equivalent to a DDIM. Dual Diffusion Implicit Bridges (DDIB) [6] concatenates two DDIMs for unpaired image-to-image translation, which forms image to latent to image concatenation of optimal transport. This empirically works well. Since DDIMs are deterministic and reversible, DDIB guaruntee cycle consistency up to ODE discretization error.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/ddib.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Dual Diffusion Implicit Bridge.</figcaption> </figure> </div> <p>Suppose our distributions are text conditioned distributions \(p(\mathbf{x}\vert\mathbf{y})\) with score \(\epsilon(\mathbf{x}_t, \mathbf{y}, t)\). The DDIB translation is expressed as</p> \[\epsilon_{\text{SBP}}^* \cong \text{ODESolve}(\mathbf{x}_{\text{src}}, \mathbf{x}_T, p(\mathbf{x}\vert\mathbf{y}_{\text{src}})) \rightarrow \text{ODESolve}(\mathbf{x}_T, \mathbf{x}_{\text{tgt}}, p(\mathbf{x}\vert\mathbf{y}_{\text{tgt}}))\] <p>One issue with DDIBs is they are slow as \(p_1\) must be the (DDIM reparametrized) prior distribution for the VE SDE i.e. fixed variance Gaussian, which means there are two full DDIM evaluations per translation. Instead, [7] proposes a one step linear approximation to DDIB as the difference of scores</p> \[\epsilon_{\text{SBP}} \cong \epsilon(\mathbf{x}_t, \mathbf{y}_{\text{tgt}}, t) - \epsilon(\mathbf{x}_t, \mathbf{y}_{\text{src}}, t)\] <p>The figure below shows the linear approximation compared to full DDIB.</p> <div style="text-align: center;"> <figure> <picture> <img src="/assets/img/blog/vision-through-scattering-medium/ddib_linear.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 90%; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Linear approximation for DDIB.</figcaption> </figure> </div> <p>In practice, we take the translation gradient as proportional to</p> \[\mathbb{E}_{t, \epsilon}\left[ \epsilon_{\phi}(x_t, y_{\text{src}}, t) - \epsilon_{\phi}(x_t, y_{\text{tgt}}, t) \right]\] <p>which is the gradient we saw above. This gradient can be applied less often than the number of UNet evaluations in DDIB and also enables compositional domain translation as shown above.</p> <p>And so far that’s it. Thanks for reading and stay tuned for more updates on vision in degraded visibility conditions!</p> <h2 id="references">References</h2> <p>[1] Koschmieder, H., Theorie der horizontalen Sichtweite. Beitr. Phys. Atmos. 12, 33-58, 1906 <br/> [2] Duntley, S. Q., The visibility of distant objects. J Opt Soc Am., 1948 <br/> [3] DehazeNet: An End-to-End System for Single Image Haze Removal, 2016 <br/> [4] Single Image Haze Removal Using Dark Channel Prior, 2009 <br/> [5] InstructPix2Pix: Learning to Follow Image Editing Instructions, 2022 <br/> [6] Dual Diffusion Implicit Bridges for Image-to-Image Translation, 2023 <br/> [7] Rethinking Score Distillation as a Bridge Between Image Distributions, 2024 <br/> [8] Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling, 2021 <br/> [9] Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory, 2022</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="machine-learning"/><summary type="html"><![CDATA[Can we remove weather effects from images? Can we do the opposite?]]></summary></entry></feed>